{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kaggle\n",
    "from pyspark.sql import SparkSession\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "def fetch_kaggle_dataset_as_dataframe(dataset_name, file_name):\n",
    "    \"\"\"\n",
    "    Fetch a specified Kaggle dataset file and return it as a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - dataset_name (str): The identifier for the dataset in format \"USERNAME/DATASET\".\n",
    "    - file_name (str): The specific file within the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the dataset's data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary directory for the Kaggle dataset\n",
    "    download_dir = \"./temp_kaggle_download\"\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "        logger.info(f\"Created directory: {download_dir}\")\n",
    "\n",
    "    # Authenticate and download the dataset\n",
    "    logger.info(f\"Authenticating and downloading dataset: {dataset_name}\")\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_name, path=download_dir, unzip=True)\n",
    "\n",
    "    # Path to the desired file within the dataset\n",
    "    file_path = os.path.join(download_dir, file_name)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.error(f\"File {file_name} not found in downloaded dataset.\")\n",
    "        return None\n",
    "\n",
    "    # Load the file into a Pandas DataFrame\n",
    "    logger.info(f\"Reading file {file_name} into DataFrame.\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Clean up (delete the temporary dataset directory)\n",
    "    logger.info(\"Cleaning up temporary files.\")\n",
    "    os.remove(file_path)\n",
    "    os.rmdir(download_dir)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_count_ordered(df_pandas):\n",
    "    \"\"\"\n",
    "    Receives a Pandas DataFrame, uses Spark to perform row count grouped by 'town',\n",
    "    orders the result by count, and returns the result as a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df_pandas (pd.DataFrame): Input Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Resultant DataFrame with counts per town ordered by count.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Initializing Spark session.\")\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"GroupByTownOrdered\").getOrCreate()\n",
    "\n",
    "    # Convert Pandas DataFrame to Spark DataFrame\n",
    "    logger.info(\"Converting Pandas DataFrame to Spark DataFrame.\")\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    # Group by 'town', count rows, and order by count\n",
    "    logger.info(\"Grouping by 'town', counting rows, and ordering by count.\")\n",
    "    grouped_df_spark = (\n",
    "        df_spark.groupBy(\"town\").count().orderBy(\"count\", ascending=False)\n",
    "    )\n",
    "\n",
    "    # Convert the result back to Pandas DataFrame\n",
    "    logger.info(\"Converting the result back to Pandas DataFrame.\")\n",
    "    result_df_pandas = grouped_df_spark.toPandas()\n",
    "\n",
    "    # Stop the Spark session\n",
    "    logger.info(\"Stopping the Spark session.\")\n",
    "    spark.stop()\n",
    "\n",
    "    return result_df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_df_to_adls2(account_name, account_key, file_system_name, file_path, df):\n",
    "    \"\"\"\n",
    "    Write a pandas DataFrame to Azure Data Lake Storage Gen2 as a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - account_name (str): Azure storage account name\n",
    "    - account_key (str): Azure storage account key\n",
    "    - file_system_name (str): Name of the file system (container) in ADLS Gen2\n",
    "    - file_path (str): Path of the file inside the file system, including its name\n",
    "    - df (pd.DataFrame): DataFrame to be written\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"Writing DataFrame to Azure Data Lake Storage Gen2 at {file_path} in {file_system_name}.\"\n",
    "    )\n",
    "\n",
    "    # Create a Data Lake service client using account name and key\n",
    "    logger.debug(\"Creating Data Lake service client.\")\n",
    "    service_client = DataLakeServiceClient(\n",
    "        account_url=f\"https://{account_name}.dfs.core.windows.net\",\n",
    "        credential=account_key,\n",
    "    )\n",
    "\n",
    "    # Get the file system client for the specified file system\n",
    "    logger.debug(f\"Getting file system client for: {file_system_name}.\")\n",
    "    file_system_client = service_client.get_file_system_client(file_system_name)\n",
    "\n",
    "    # Get the data lake file client for the specified file path\n",
    "    logger.debug(f\"Getting file client for: {file_path}.\")\n",
    "    file_client = file_system_client.get_file_client(file_path)\n",
    "\n",
    "    # Convert the dataframe to CSV format and get the content in bytes\n",
    "    logger.debug(\"Converting DataFrame to CSV format.\")\n",
    "    csv_content = StringIO()\n",
    "    df.to_csv(csv_content, index=False)\n",
    "    csv_bytes = csv_content.getvalue().encode(\"utf-8\")\n",
    "\n",
    "    # Upload the content to the file\n",
    "    logger.info(\"Uploading CSV content to Azure Data Lake Storage Gen2.\")\n",
    "    file_client.upload_data(csv_bytes, overwrite=True)\n",
    "    logger.success(f\"Successfully uploaded DataFrame to {file_path}.\")\n",
    "# Example Usage\n",
    "# df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n",
    "# write_df_to_adls2('YOUR_ACCOUNT_NAME', 'YOUR_ACCOUNT_KEY', 'YOUR_FILE_SYSTEM_NAME', 'path/to/yourfile.csv', df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 17:45:01.838 | INFO     | __main__:fetch_kaggle_dataset_as_dataframe:26 - Created directory: ./temp_kaggle_download\n",
      "2023-10-22 17:45:01.840 | INFO     | __main__:fetch_kaggle_dataset_as_dataframe:29 - Authenticating and downloading dataset: anoopjohny/real-estate-sales-2001-2020-state-of-connecticut\n",
      "2023-10-22 17:45:09.310 | INFO     | __main__:fetch_kaggle_dataset_as_dataframe:41 - Reading file Real_Estate_Sales_2001-2020_GL.csv into DataFrame.\n",
      "/tmp/ipykernel_848/2141684446.py:42: DtypeWarning: Columns (8,9,10,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "2023-10-22 17:45:11.073 | INFO     | __main__:fetch_kaggle_dataset_as_dataframe:45 - Cleaning up temporary files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines present:- 997213\n"
     ]
    }
   ],
   "source": [
    "# Example Usage: Fetch a specified Kaggle dataset file and return it as a Pandas DataFrame.\n",
    "dataset_id = \"anoopjohny/real-estate-sales-2001-2020-state-of-connecticut\"  # Use your desired dataset's identifier\n",
    "file_in_dataset = \"Real_Estate_Sales_2001-2020_GL.csv\"  # Use the specific file name you want within the dataset\n",
    "dataframe = fetch_kaggle_dataset_as_dataframe(dataset_id, file_in_dataset)\n",
    "print(\"Number of lines present:-\",  \n",
    "      len(dataframe)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 17:45:11.288 | INFO     | __main__:group_and_count_ordered:13 - Initializing Spark session.\n",
      "2023-10-22 17:45:11.486 | INFO     | __main__:group_and_count_ordered:18 - Converting Pandas DataFrame to Spark DataFrame.\n",
      "/home/tarcisio/miniconda3/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:479: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n",
      "2023-10-22 17:45:43.259 | INFO     | __main__:group_and_count_ordered:22 - Grouping by 'town', counting rows, and ordering by count.\n",
      "2023-10-22 17:45:43.291 | INFO     | __main__:group_and_count_ordered:28 - Converting the result back to Pandas DataFrame.\n",
      "23/10/22 17:45:43 WARN TaskSetManager: Stage 0 contains a task of very large size (7088 KiB). The maximum recommended task size is 1000 KiB.\n",
      "2023-10-22 17:45:46.526 | INFO     | __main__:group_and_count_ordered:32 - Stopping the Spark session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             town  count\n",
      "0      Bridgeport  34201\n",
      "1        Stamford  32529\n",
      "2       Waterbury  28506\n",
      "3         Norwalk  23960\n",
      "4       New Haven  21346\n",
      "5         Danbury  20350\n",
      "6   West Hartford  19854\n",
      "7        Hartford  18810\n",
      "8         Milford  17749\n",
      "9         Meriden  17502\n",
      "10      Greenwich  17390\n",
      "11        Bristol  16915\n",
      "12      Stratford  16688\n",
      "13    New Britain  16405\n",
      "14     Manchester  16380\n",
      "15         Hamden  16192\n",
      "16      Fairfield  15898\n",
      "17  East Hartford  13732\n",
      "18     Torrington  13172\n",
      "19     Middletown  12403\n"
     ]
    }
   ],
   "source": [
    "# Example Usage: Receives a Pandas DataFrame, uses Spark to perform row count grouped by 'town', orders the result by count, and returns the result as a Pandas DataFrame.\n",
    "df = pd.DataFrame(dataframe)\n",
    "result = group_and_count_ordered(df)\n",
    "print(result.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 17:45:47.535 | INFO     | __main__:write_df_to_adls2:12 - Writing DataFrame to Azure Data Lake Storage Gen2 at stest.csv in montrealfilesystem.\n",
      "2023-10-22 17:45:47.537 | DEBUG    | __main__:write_df_to_adls2:17 - Creating Data Lake service client.\n",
      "2023-10-22 17:45:47.538 | DEBUG    | __main__:write_df_to_adls2:24 - Getting file system client for: montrealfilesystem.\n",
      "2023-10-22 17:45:47.540 | DEBUG    | __main__:write_df_to_adls2:28 - Getting file client for: stest.csv.\n",
      "2023-10-22 17:45:47.541 | DEBUG    | __main__:write_df_to_adls2:32 - Converting DataFrame to CSV format.\n",
      "2023-10-22 17:45:50.645 | INFO     | __main__:write_df_to_adls2:38 - Uploading CSV content to Azure Data Lake Storage Gen2.\n",
      "2023-10-22 17:46:05.982 | SUCCESS  | __main__:write_df_to_adls2:40 - Successfully uploaded DataFrame to stest.csv.\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Saves a Pandas DataFrame to Azure Data Lake Storage Gen2.\n",
    "write_df_to_adls2(\n",
    "    account_name=\"montrealadls\",\n",
    "    account_key=\"dWksQ33gDM56isvYdBv0U/lrOSwK5QQPfLRKCKJagYBhc0pR4UIb2GpPj+tvMT6oFUX24J/fi8lv+AStybQh1g==\",\n",
    "    file_system_name=\"montrealfilesystem\",\n",
    "    file_path=\"stest.csv\",\n",
    "    df=df\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
