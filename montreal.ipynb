{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kaggle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import pandas as pd\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "\n",
    "def fetch_kaggle_dataset_as_dataframe(dataset_name, file_name):\n",
    "    \"\"\"\n",
    "    Fetch a specified Kaggle dataset file and return it as a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - dataset_name (str): The identifier for the dataset in format \"USERNAME/DATASET\".\n",
    "    - file_name (str): The specific file within the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the dataset's data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a temporary directory for the Kaggle dataset\n",
    "    download_dir = \"./temp_kaggle_download\"\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    # Authenticate and download the dataset\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_name, path=download_dir, unzip=True)\n",
    "\n",
    "    # Path to the desired file within the dataset\n",
    "    file_path = os.path.join(download_dir, file_name)\n",
    "    \n",
    "    # Load the file into a Pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Clean up (delete the temporary dataset directory)\n",
    "    os.remove(file_path)\n",
    "    os.rmdir(download_dir)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "def group_and_count_ordered(df_pandas):\n",
    "    \"\"\"\n",
    "    Receives a Pandas DataFrame, uses Spark to perform row count grouped by 'town', \n",
    "    orders the result by count, and returns the result as a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df_pandas (pd.DataFrame): Input Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Resultant DataFrame with counts per town ordered by count.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"GroupByTownOrdered\").getOrCreate()\n",
    "\n",
    "    # Convert Pandas DataFrame to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    # Group by 'town', count rows, and order by count\n",
    "    grouped_df_spark = df_spark.groupBy(\"town\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "    # Convert the result back to Pandas DataFrame\n",
    "    result_df_pandas = grouped_df_spark.toPandas()\n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n",
    "    return result_df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_adls2(df, account_name, account_key, filesystem_name, file_path):\n",
    "    \"\"\"\n",
    "    Saves a Pandas DataFrame to Azure Data Lake Storage Gen2.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The DataFrame to save.\n",
    "    - account_name (str): The ADLS Gen2 account name.\n",
    "    - account_key (str): The ADLS Gen2 account key.\n",
    "    - filesystem_name (str): The name of the filesystem (equivalent to a container in Blob storage).\n",
    "    - file_path (str): The path where the file should be saved, including the filename (e.g., \"folder/data.csv\").\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DataFrame to CSV string\n",
    "    csv_data = df.to_csv(index=False)\n",
    "\n",
    "    # Establish a connection to ADLS Gen2\n",
    "    service_client = DataLakeServiceClient(account_url=f\"https://{account_name}.dfs.core.windows.net\", \n",
    "                                           credential=account_key)\n",
    "\n",
    "    # Get the filesystem client\n",
    "    filesystem_client = service_client.get_file_system_client(filesystem_name)\n",
    "\n",
    "    # Get the file client and upload data\n",
    "    file_client = filesystem_client.get_file_client(file_path)\n",
    "    file_client.upload_data(csv_data, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage: Fetch a specified Kaggle dataset file and return it as a Pandas DataFrame.\n",
    "dataset_id = \"anoopjohny/real-estate-sales-2001-2020-state-of-connecticut\"  # Use your desired dataset's identifier\n",
    "file_in_dataset = \"Real_Estate_Sales_2001-2020_GL.csv\"  # Use the specific file name you want within the dataset\n",
    "dataframe = fetch_kaggle_dataset_as_dataframe(dataset_id, file_in_dataset)\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage: Receives a Pandas DataFrame, uses Spark to perform row count grouped by 'town', orders the result by count, and returns the result as a Pandas DataFrame.\n",
    "df = pd.DataFrame(dataframe)\n",
    "result = group_and_count_ordered(df)\n",
    "print(result.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Saves a Pandas DataFrame to Azure Data Lake Storage Gen2.\n",
    "save_dataframe_to_adls2(\n",
    "    df,\n",
    "    account_name=\"montrealadls\",\n",
    "    account_key=\"mmWuCeIjJ6f4nRgI06hKio3HaFGQvdke4oH3ZI8nlKvY8gWc7/lUUs1Ne+QYmYQRgICKMUtPshHR+ASt22wlkg==\",\n",
    "    filesystem_name=\"montrealfilesystem\",\n",
    "    file_path=\"stest.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
