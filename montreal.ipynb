{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kaggle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "\n",
    "def fetch_kaggle_dataset_as_dataframe(dataset_name, file_name):\n",
    "    \"\"\"\n",
    "    Fetch a specified Kaggle dataset file and return it as a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - dataset_name (str): The identifier for the dataset in format \"USERNAME/DATASET\".\n",
    "    - file_name (str): The specific file within the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the dataset's data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a temporary directory for the Kaggle dataset\n",
    "    download_dir = \"./temp_kaggle_download\"\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    # Authenticate and download the dataset\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_name, path=download_dir, unzip=True)\n",
    "\n",
    "    # Path to the desired file within the dataset\n",
    "    file_path = os.path.join(download_dir, file_name)\n",
    "    \n",
    "    # Load the file into a Pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Clean up (delete the temporary dataset directory)\n",
    "    os.remove(file_path)\n",
    "    os.rmdir(download_dir)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "def group_and_count_ordered(df_pandas):\n",
    "    \"\"\"\n",
    "    Receives a Pandas DataFrame, uses Spark to perform row count grouped by 'town', \n",
    "    orders the result by count, and returns the result as a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df_pandas (pd.DataFrame): Input Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Resultant DataFrame with counts per town ordered by count.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"GroupByTownOrdered\").getOrCreate()\n",
    "\n",
    "    # Convert Pandas DataFrame to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    # Group by 'town', count rows, and order by count\n",
    "    grouped_df_spark = df_spark.groupBy(\"town\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "    # Convert the result back to Pandas DataFrame\n",
    "    result_df_pandas = grouped_df_spark.toPandas()\n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n",
    "    return result_df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_adls2(account_name, account_key, file_system_name, file_path, df):\n",
    "    \"\"\"\n",
    "    Write a pandas DataFrame to Azure Data Lake Storage Gen2 as a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - account_name (str): Azure storage account name\n",
    "    - account_key (str): Azure storage account key\n",
    "    - file_system_name (str): Name of the file system (container) in ADLS Gen2\n",
    "    - file_path (str): Path of the file inside the file system, including its name\n",
    "    - df (pd.DataFrame): DataFrame to be written\n",
    "    \"\"\"\n",
    "    # Create a Data Lake service client using account name and key\n",
    "    service_client = DataLakeServiceClient(account_url=f\"https://{account_name}.dfs.core.windows.net\",\n",
    "                                           credential=account_key)\n",
    "\n",
    "    # Get the file system client for the specified file system\n",
    "    file_system_client = service_client.get_file_system_client(file_system_name)\n",
    "\n",
    "    # Get the data lake file client for the specified file path\n",
    "    file_client = file_system_client.get_file_client(file_path)\n",
    "\n",
    "    # Convert the dataframe to CSV format and get the content in bytes\n",
    "    csv_content = StringIO()\n",
    "    df.to_csv(csv_content, index=False)\n",
    "    csv_bytes = csv_content.getvalue().encode('utf-8')\n",
    "\n",
    "    # Upload the content to the file\n",
    "    file_client.upload_data(csv_bytes, overwrite=True)\n",
    "\n",
    "# Example Usage\n",
    "# df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n",
    "# write_df_to_adls2('YOUR_ACCOUNT_NAME', 'YOUR_ACCOUNT_KEY', 'YOUR_FILE_SYSTEM_NAME', 'path/to/yourfile.csv', df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage: Fetch a specified Kaggle dataset file and return it as a Pandas DataFrame.\n",
    "dataset_id = \"anoopjohny/real-estate-sales-2001-2020-state-of-connecticut\"  # Use your desired dataset's identifier\n",
    "file_in_dataset = \"Real_Estate_Sales_2001-2020_GL.csv\"  # Use the specific file name you want within the dataset\n",
    "dataframe = fetch_kaggle_dataset_as_dataframe(dataset_id, file_in_dataset)\n",
    "print(\"Number of lines present:-\",  \n",
    "      len(dataframe)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage: Receives a Pandas DataFrame, uses Spark to perform row count grouped by 'town', orders the result by count, and returns the result as a Pandas DataFrame.\n",
    "df = pd.DataFrame(dataframe)\n",
    "result = group_and_count_ordered(df)\n",
    "print(result.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Saves a Pandas DataFrame to Azure Data Lake Storage Gen2.\n",
    "write_df_to_adls2(\n",
    "    account_name=\"montrealadls\",\n",
    "    account_key=\"dWksQ33gDM56isvYdBv0U/lrOSwK5QQPfLRKCKJagYBhc0pR4UIb2GpPj+tvMT6oFUX24J/fi8lv+AStybQh1g==\",\n",
    "    file_system_name=\"montrealfilesystem\",\n",
    "    file_path=\"stest.csv\",\n",
    "    df=df\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
